<!DOCTYPE html>
<html>
  <head>
    <title>Hypothesis Testing</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
    </style>
    <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

Elements of Data Science - S2020

# Hypothesis Testing

---
# Hypothesis Testing

--
count:false
- Random Sampling
--
count:false
- Confidence Intervals
--
count:false
- A/B Tests
--
count:false
- Hypothesis Testing
--
count:false
- Permutation Tests
--
count:false
- p-values
--
count:false
- Calculating Power
--
count:false
- Multi-Armed Bandit

---
# Questions and more questions

--
count:false
- Have web conversions gone up?
--
count:false
- Have stock prices changed?
--
count:false
- Which ad generates more sales?
--
count:false
- Which headline generates more clicks?
--
count:false
- Did the number of likes change?

---
#Mini Probability Review 

- Random Variable 
    - takes values from an associated probability distribution

--
count:false
- Distribution
    - describes probability of values of a Random Variable

--
count:false
- $P(x)$: Probability
    - probability of seeing $x$, takes value in [0,1]
    - Ex: Probability of getting heads on a coin toss


--
count:false
- $P(x|y)$: Conditional Probability
    - probability of seeing $x$, given that some $y$ is true
    - Ex: Probability of getting heads on a coin toss given that coin is fair



--
- so much more! See [Data Science From Scratch](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3dnlebk%26AN%3d979529%26site%3dehost-live%26scope%3dsite&ebv=EK&ppid=Page-__-54)
---
#Population Distributions and Sampling

<br/>
--
count:false
- **The World :: Ground Truth**
    - Ex: How taxi rides work

<br/>
--
count:false
- **Our Data :: An Experiment**
    - Ex: The taxi rides we saw in Jan 2017

---
#Population Dists. and Sampling
<br>
--
count:false
- **Population Distribution:** The actual distribution out in world
    - Ex: Actual distribution of taxi trip length

--
count:false
- **Random Sample:** Our observations of the true population distrution 
--
count:false
    - We hope this does not differ systematically from the true distribution
--
count:false
    - Ex: The taxi trip lengths recorded in Jan 2017

--
count:false
- **Sample Size (n):** The number of observations, the larger the better
    - Ex: We saw 10,000 trips

---
#Population Dists and Sampling
<br>
--
count:false
- **Sample Statistic:** eg. mean, median, standard deviation
    - Ex: We're interested in mean trip length

--
count:false
- **Sampling Distribution:** Distribution of the sample statistic
    - Ex: How is mean trip length distributed?

--
count:false
- **Population Mean vs. Sample Mean:**  $\mu$  vs.  $\bar{x}$
    - Ex: The true mean trip length vs the one we observed

--
count:false
- **Population Std. Dev. vs Sample Std. Dev.:**  $\sigma$  vs.  $s$
    - Ex: The true spread of trip length vs the one we observed


---
# Things To Know First

- sample size
- shape
- location (central tendencies)
- spread


---
# Taxi Example

```python
df = pd.read_csv('../data/yellow_tripdata_2017-01_subset10000rows.csv')

# imagine this is our population distribution
trip_distance = df.trip_distance.dropna().iloc[:1000]
```

---
# Sample of Taxi Data
.smaller[
```python
np.random.seed(123)

n = 50 # sample size

# take a sample from the population
sample_idx = np.random.permutation(len(trip_distance))[:n]
sample = trip_distance.iloc[sample_idx]
sample.describe()
```]
.smaller[
```
count    50.000000
mean      2.290800
std       2.596723
min       0.370000
25%       0.825000
50%       1.450000
75%       2.550000
max      13.100000
Name: trip_distance, dtype: float64
```]

---
# Sample of Taxi Data

Can also sample using pandas:

.smaller[
```python
sample = trip_distance.sample(n, random_state=123)
sample.describe()
```]
.smaller[
```
count    50.000000
mean      2.290800
std       2.596723
min       0.370000
25%       0.825000
50%       1.450000
75%       2.550000
max      13.100000
Name: trip_distance, dtype: float64
```]


---
# Plot Sample

```python
fig,ax = plt.subplots(1,2,figsize=(12,4))
sns.distplot(sample, kde=False, rug=True, ax=ax[0]);
sns.boxplot(sample, ax=ax[1]);
```
.center[![:scale 100%](images/tripdistance_distplot.png)]


---
# Define the Sample Statistic

```python
xbar = sample.mean()
print(f'sample mean: {xbar:0.2f}')
```
```
sample mean: 2.29
```

--
count:false
- How good of an approximation is our sample statistic?
--
count:false
- Let's take more samples!

---
# Generating Samples

```python
sample_means = []
for i in range(1000):
    sample_mean = trip_distance.sample(n,random_state=i).mean()
    sample_means.append(sample_mean)
```

---
# Sampling Distribution
.smaller[
```python
# sampling distribution with original statistic
ax = sns.distplot(sample_means, kde=False)
ax.set_xlabel('sample_means');
ax.set_ylabel('frequency');
ax.vlines(xbar,*ax.get_ylim());
```]
.center[![:scale 50%](images/tripdistance_samplemeans_distplot.png)]


---
# Central Limit Theorem

<br>
--
count:false
If all samples are randomly drawn from the same sample population:

<br>
--
count:false
For reasonably large samples (usually $n \ge 30$), the distribution of sample mean $\bar{x}$ is normal regardless of the distribution of $X$.

<br>
--
count:false
The sampling distribution of $\bar{x}$ becomes approximately normal as the the sample size $n$ gets large.

<br>
Ex: $X$ = trip_distance, $\bar{x}$ = mean trip_distance, $n$ = 50

---
# What is Normal?

<br>
--
count:false
- distribution defined by mean ($\mu$) and standard deviation ($\sigma$)

<br>
--
count:false
- $N(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma}{(x-\mu)}^2}$

<br>
--
count:false
- **PDF** (Probability Density Function): function of a continuous random variable that provides a relative likelihood of seeing a particular sample of a random variable.


---
# Properties of a Normal Distribution

.center[
![:scale 80%](images/normal_distribution_proportions.png)]

.smallest[
from https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2]

---
# Plotting a Standard Normal

.smaller[
- Standard Normal: $\mu = 0, \sigma = 1$
- Often referred to as $Z$
]

--
count:false
.smallest[
```python
*import scipy as sp
x = np.random.normal(0,1,size=100000)
ax = sns.distplot(x);
ax.set_xlabel('x');ax.set_ylabel('N(x;0,1)')
ax.vlines([-1,1],0,sp.stats.norm.pdf(1), colors='k');
ax.vlines([-2,2],0,sp.stats.norm.pdf(2), colors='r');
```]
.center[![:scale 40%](images/normal_distplot.png)]


---
# Confidence Intervals

- Typically we only have one sample

--
count:false
.smaller[
```python
# treat all observations as our sample
n = len(trip_distance)
n
```
```
1000
```]
--
count:false
.smaller[
```python
x_bar = trip_distance.mean()
print(f'sample mean: {x_bar:0.4f}')
```
```
sample mean: 2.8324
```]

--
count:false
- What is the spread of our sample statistic?
--
count:false
- What other values might it take?

---
# Generate Confidence Intervals

## Bootstrap Confidence Interval: sampling with replacement

--
count:false
1. draw a random sample of size *n* from the data
--
count:false
2. record the sample statistic from this random sample
--
count:false
3. repeat 1 and 2 many times
--
count:false
4. for an $x\%$ conf. int., trim off $\frac{1}{2}(100-x)\%$ of the data from both ends
--
count:false
6. those trim points are the endpoints of the the $x\%$ bootstrap conf. interval

---
# Bootstrap Sampling

Sampling with replacement

--
count:false
.smaller[
```python
def generate_bootstrap(X, n=None, random_state=None):
    if random_state:
        np.random.seed(random_state)

    n = len(X) if n is None else n

    return np.random.choice(X,n,replace=True)
```]
--
count:false
.smallest[
```python
>>> generate_bootstrap(trip_distance,3,random_state=123)
```
```
array([1.6 , 0.56, 0.44])
```]

--
count:false
.smallest[
```python
>>> trip_distance.sample(3,replace=True,random_state=123) # using pandas
```
```
510    1.60
365    0.56
382    0.44
Name: trip_distance, dtype: float64
```]

---
# Generating Bootstrap Samples
```python
# 3. repeat 1 and 2 many times
num_iterations = 1000

bootstrap_means = []
for i in range(num_iterations):

    # 1. draw a random sample of size *n* from the data
    bootstrap = generate_bootstrap(trip_distance.values, random_state=i)

    # 2. record the sample statistic from this random sample
    bootstrap_means.append(np.mean(bootstrap))

bootstrap_means = np.array(bootstrap_means)
```

---
# Calculate Conf Intervals

.smaller[
```python
# 4. for an 95% conf. int., trim off .5*(1-.95) of the data from both ends
```
]
.smaller[
```python
# calculate where to trim
trim = .5*(1-.95) * num_iterations

# find the closest integer
trim = int(np.round(trim))
trim
```]
.smaller[
```
25
```]

.smaller[
```python
# 5. those trim points are the endpoints of the the  alpha%  bootstrap conf int
ci = np.sort(bootstrap_means)[[trim,-trim]] # sort the array!
ci
```]
.smaller[
```
array([2.62998, 3.08587])
```]

---
# Plotting Distribution With CIs

.smallest[
```python
ax = sns.distplot(bootstrap_means)
ylim = ax.get_ylim()
ax.vlines(trip_distance.mean(), *ylim, color='r');
ax.vlines(ci, *ylim, color='b')
ax.set_xlabel('bootstrap sample means');
ax.set_ylabel('frequency');
```]
.center[![:scale 50%](images/tripdistance_bootstrap_distplot_withconfints.png)]

---
# Plot Measure with CIs

.smallest[
```python
sns.barplot(trip_distance,
            estimator=np.mean, #default
            ci=95,             #default
            n_boot=100,        #default
            orient='v',
            color='c',
           );
```]
.center[![](images/tripdistance_bootstrap_bar_withconfints.png)]


---
# Interpreting CIs
<br>

--
count:false
Tells us something about the **variablity** of this statistic.

--
count:false
Tells us how **confident** we should be that our parameter lies in the interval.

--
count:false
Does **NOT** tell us: "the prob. the true value lies within that interval".

--
count:false
> If confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.

---
class:middle

# Questions re CIs?

---
# A/B Tests
<br>

--
count:false
##Do one of two treatments produce superior results?


--
count:false
- testing two prices to determine which generates more profit
 

--
count:false
- testing two web headlines to determine which produces more clicks


--
count:false
- testing two advertisements to see which produces more conversions

<br>
--
count:false
##Often Used Test Statistics
--
count:false
- difference in means
--
count:false
- difference in counts

---
# Hypothesis Testing


--
count:false
- Ex: Does one webpage lead to more sales than another?


--
count:false
- **Null Hypothesis:** $H_0$
    - the thing we're observing is happening due to random chance
    - there are no differences between two groups
    - Ex: A difference in sales is just random


--
count:false
- **Alternative Hypothesis:** $H_1$
    - the thing we're observing is happening **not** due to random chance
    - there is a difference between two groups
    - Ex: A difference in sales is not just random


--
count:false
- **Experiment**: given data, do we **accept or reject $H_0$**?
    - Ex: if we collect sales can we say that a difference between the two pages isn't random?

---
# Errors in Hypothesis Tests
.center[![:scale 80%](images/TypeI_TypeII.jpeg)]
.smallest[
from https://analyticsdemystified.com/analytics-strategy/type-i-vs-type-ii-errors-in-customer-data-management/]
---

# Errors in Hypothesis Tests
.center[![](images/Type-I-and-II-errors_pregnant.jpg)]

.smallest[
from https://flowingdata.com/2014/05/09/type-i-and-ii-errors-simplified/]

---
# Significance and Power

<br>
--
count:false
- $P\left(\text{reject } H_0 \mid H_0 \text{ true}\right)$ = significance of test (Type I Error)
    - Probablity of saying **things aren't by chance when they are**

<br>
--
count:false
- $P\left(\text{reject } H_0 \mid H_1 \text{ true}\right)$ = power of test (1-Type II Error)
    - Probability of saying **things aren't by chance when they aren't**
 
---
# Ex: Webpages and Sales

--
count:false
- **Question:** Which webpage leads to more sales?

    - Potential Issue: what if sales are large but infrequent?
    

--
count:false
- **Proxy Variable**: stand in for true value of interest

    - Ex: Assume 'time on page' is correlated with sales

---
# Ex: Webpages and Sales

--
count:false
.smaller[
```python
session_times = pd.read_csv('../data/web_page_data.csv')
session_times.shape
```
```
(36, 2)
```
```python
session_times.head(3)
```
```
	Page	Time
0	Page A	12.6
1	Page B	151.8
2	Page A	21.0
```]

---
# Ex: Plotting Distributions

```python
sns.boxplot(x='Page',y='Time',data=session_times);
```
.center[![:scale 60%](images/pagetime_boxplots.png)]

---
# Ex: Plotting Mean and CIs

```python
sns.barplot(x='Page',y='Time',data=session_times);
```
.center[![:scale 60%](images/pagetime_bar.png)]


---
# Ex: Define Metric

--
count:false
- **Metric:** the measure we're interested in
--
count:false
- Ex: We're interested in a difference of means (Page B - Page A)

--
count:false
.smaller[
```python
mean_a = session_times[session_times.Page == 'Page A'].Time.mean()
mean_b = session_times[session_times.Page == 'Page B'].Time.mean()

observed_metric = mean_b-mean_a
print('observed metric: {:0.2f}'.format(observed_metric))
```
```
observed metric: 21.40
```]


--
count:false
## Is this surprising? Should we reject the null?

- Assuming that $H_0$ is true, is this observation surprising?


---
# Permutation Test

--
count:false
- Recall Central Limit Theorem
> For reasonably large samples, the distribution of sample mean $\bar{x}$ has is normal regardless of the distribution of $X$.

--
count:false
- How do generate additional samples? Resampling!

---
# Permutation Test

--
count:false
1. combine groups together (assume $H_0$ is true)
    - Ex: Ignore Page 
--
count:false
1. permute observations
    - Ex: Reorder Time 
--
count:false
1. create new groups and calcuate statistic (same sizes as originals)
    - Ex: Get set1 of Times of size |PageA| and set2 of size |PageB|
--
count:false
1. calculate metric
    - Ex: mean(set 2) - mean(set 1)
--
count:false
1. repeat many times
--
count:false
1. see where our original observation falls

---
# Ex: Permutation Test

--
count:false
.smaller[
```python
# 0. get group sizes
n_a = sum(session_times.Page == 'Page A')
n_b = sum(session_times.Page == 'Page B')
n = n_a + n_b
```]

--
count:false
.smaller[
```python
# 1. combine groups together (assume $H_0$ is true)
samples = session_times.Time
```]

--
count:false
.smaller[
```python
# 2. permute observations
permuted = session_times.Time.sample(n,replace=False,random_state=123)
```]

--
count:false
.smaller[
```python
# 3. calculate metric
rand_mean_a = permuted[:n_a].mean()
rand_mean_b = permuted[n_a:].mean()
rand_mean_diff = (rand_mean_b - rand_mean_a)
print('{:.2f}'.format(rand_mean_diff))
```
```
-17.41
```]

---
# Ex: Permutation Test

--
count:false
.smaller[
```python
# 4. repeat many times
rand_mean_diffs = []
for i in range(10000):
    permuted = session_times.Time.sample(n,replace=False,random_state=i)
    rand_mean_a = permuted[:n_a].mean()
    rand_mean_b = permuted[n_a:].mean()
    rand_mean_diffs.append(rand_mean_b - rand_mean_a)
rand_mean_diffs[:5]
```
```
[16.257142857142853,
 16.874285714285705,
 10.291428571428597,
 -31.811428571428564,
 10.565714285714321,
 -6.508571428571415,
 12.48571428571428,
 -7.605714285714299,
 -16.794285714285706,
 -6.577142857142874]
```]

---
# Ex: Permutation Test

```python
# 5. see where our original observation falls
ax = sns.distplot(rand_mean_diffs, norm_hist=False, kde=False)
ax.set_xlabel('random mean differences');ax.set_ylabel('frequency');
ax.vlines(observed_metric, *ax.get_ylim(), color='r');
```
.center[![:scale 50%](images/pagetime_permutation_test.png)]


---
#Normalization: z-score

- Convert our distribution to an approximation of standard normal

--count:
false
1. shift mean to 0
2. standard deviation of 1

$\Large z = \frac{x - \bar{x}}{s}$

--
count:false
```python
xbar = np.mean(rand_mean_diffs)
s = np.std(rand_mean_diffs)
```
--
count:false
```python
rand_zscores = (rand_mean_diffs - xbar) / s
```

--
count:false
```python
observed_metric_zscore =  (observed_metric - xbar) / s 
```

---
# Ex: Permutation Test

.smaller[
```python
# 5. see where our original observation falls (normalized)
ax = sns.distplot(rand_zscores, norm_hist=False, kde=False)
ax.set_xlabel('random mean differences normed');ax.set_ylabel('frequency');
ax.vlines(observed_metric_zscore, *ax.get_ylim(), color='r');
```]
.center[![:scale 55%](images/pagetime_permutation_test_normed.png)]


---
# Why Permutation Test?
<br>

--
count:false
- data can be numeric or binary


--
count:false
- sample sizes can be different


--
count:false
- assumptions about normally distributed data are not needed

---
# How sure are we?

--
count:false
- p-values

> The probability of finding the observed, or more extreme, results when the null hypothesis ($H_0$) is true.

--
count:false


--
count:false
- does mean : $P\left(\text{data} \mid H_0 \text{ is true}\right)$


--
count:false
- does NOT mean : $P\left(H_0 \text{ is not true} \mid \text{data}\right)$


--
count:false
- Our question about significance becomes:

> "How often did we see a value as or more extreme than our observed metric?"

---
# Calculating $p$

--
count:false
```python
# find absolute values greater than our observed_metric
gt = np.abs(np.array(rand_mean_diffs)) >= np.abs(observed_metric)
```
--
count:false
```python
# how many are greater?
num_gt = gt.sum()
```
--
count:false
```python
# proportion of total that are as or more extreme
p = num_gt / len(rand_mean_diffs)
p
```
--
count:false
```
0.2655
```

---
# Tails

.center[![:scale 100%](images/one_vs_two_tailed.gif)]

.smallest[From https://towardsdatascience.com/one-tailed-or-two-tailed-test-that-is-the-question-1283387f631c]


---
#One-Tailed vs Two-Tailed
<br>

--
count:false
- Do we have a strong reason for a one-tailed? One-Tailed
    - Ex: H_0 is "difference is less than or equal to 0"
    - Need a strong reason

<br>
--
count:false
- Otherwise? Two-tailed
    - Ex: H_0 is "there is no real difference between groups"
    - More conservative
    - Usually a better choice


---
#One-Tailed Test
<br>

.smaller[
```python
sum(np.array(rand_mean_diffs) >= observed_metric) / len(rand_mean_diffs)
```
```
0.1311
```]

--
count:false
Note that this is less than our Two-Tailed value!


---
#Choosing $\alpha$

--
count:false
- **alpha ($\alpha$):** significance level
    - What we'll compare our p-value to
    - Best to choose this before calculating metrics
    - Probability of rejecting the null when it is true (Type I Error)

--
count:false
- Usually .05 (Error 1 out of 20 times)


--
count:false
- .01 (Error 1 out of 100 times)


--
count:false
- .1 (Error 1 out of 10 times)


--
count:false
- Depends on how bad a Type I (False Positive) Error is


---
#Another Ex: Price vs Conversion

- Does Price A lead to higher conversions than Price B?

--
count:false

- **Conversion:** Turning a visit into a sale


--
count:false
- $H_0$: conversions for Price A &le; conversions for Price B
--
count:false
- $H_1$: conversions for Price A &gt; conversions for Price B


--
count:false
```python
df = pd.DataFrame({'Price A':[200,23539],
                   'Price B':[182,22406]},
                   index=['Conversion','No Conversion'])
df
```
```
             Price A Price B
Conversion       200     182
No Conversion  23539   22406
```


---
#Another Ex: Price vs Conversion

- Metric of Interest?
--
count:false
    - difference in percent conversion

--
count:false
```python
pct_conv = df.loc['Conversion'] / df.sum(axis=0) * 100
pct_conv
```
```
Price A    0.842495
Price B    0.805738
dtype: float64
```
--
count:false
```python
diff_pct_conv = pct_conv['Price A'] - pct_conv['Price B']
print(f'{diff_pct_conv:.4f}%')
```
```
0.0368%
```

---
#Another Ex: Price vs Conversion

--
count:false
- First: Choose our $\alpha$: 0.05


--
count:false
- Reminder of Permutation Test:
--
count:false

    0. get group sizes
    1. combine groups together
    2. permute observations
    3. grab two new samples (same sizes as originals)
    4. calculate metric
    5. repeat many times
    6. see where our original observation falls

---
#Another Ex: Price vs Conversion

- What are our samples?

   -  1 = Conversion
   -  0 = No conversion

--
count:false
- How many samples are there?
```python
n = df.sum().sum()
n
```
46327


---
#Another Ex: Price vs Conversion

--
count:false
- Turning counts into samples

--
count:false
.smaller[
```python
n_conversion = df.loc['Conversion'].sum()
n_conversion
```
```
382
```]

--
count:false
.smaller[
```python
samples = np.zeros(n)
samples[:n_conversion] = 1  

assert sum(samples) == n_conversion
```]

---
#Another Ex: Price vs Conversion

--
count:false
```python
n_a, n_b = df.sum(axis=0)
print(n_a, n_b, n_a + n_b)

assert n_a + n_b == n
```
```
23739 22588 46327
```

--
count:false
```python
%%time
np.random.seed(123)
rand_conv_diffs = []
for i in tqdm.tqdm(range(1000)):
    permutation = np.random.permutation(samples)
    rand_conv_a = sum(permutation[:n_a]) / n_a
    rand_conv_b = sum(permutation[n_a:]) / n_b
    rand_conv_diffs.append(100 * (rand_conv_a - rand_conv_b))
```
```
CPU times: user 6.39 s, sys: 11.5 ms, total: 6.4 s
Wall time: 6.39 s
```

---
#Another Ex: Price vs Conversion

--
count:false
.smaller[
```python
ax = sns.distplot(rand_conv_diffs, norm_hist=False, kde=False)
ax.vlines(diff_pct_conv, *ax.get_ylim(), color='r');
```]
.center[![:scale 35%](images/conversion_permutation_distplot.png)]

--
count:false
.smaller[
```python
# calculate a two-tailed p-value
( sum(np.abs(np.array(rand_conv_diffs)) >= np.abs(diff_pct_conv))
    / len(rand_conv_diffs) )
```
```
0.663
```]

---
# Proportion Test

```python
from statsmodels.stats.proportion import proportions_ztest

z,p = proportions_ztest(df.loc['Conversion'].values,
                        df.sum(),
                        alternative='two-sided')
p
```
```
0.6618881488312065
```



---
# t-Test: Equation Based

- based on the Student-t distribution
- more involved to describe
- works for numeric data (can't use it for the last example)

--
count:false
.smaller[
```python
# using our session_times example
t = sp.stats.ttest_ind(session_times[session_times.Page == 'Page A'].Time.values,
                       session_times[session_times.Page == 'Page B'].Time.values,
                       equal_var=False)
t.pvalue
```
```
0.28152437245700607
```]

---
# How many observations?

- **Common question:** How many observations do we need?

--
count:false
- These 4 things are related:
--
count:false
    - **effect size:** Min size of effect you want to detect 
        - Ex: "%10 increase in clicks"
--
count:false
    - **power**: Prob. of detecting given effect size with given sample size 
        - $P(\text{reject } H_0 \mid H_1 \text{ true})$
--
count:false
    - **alpha**: significance level at which to reject the null
        - $1-P(\text{reject }H_0 \mid H_0 \text{ true})$
--
count:false
    - **number of observations:** usually what we're interested in finding


--
count:false
- Knowing 3 gets you the 4th

---
# How many observations?
.smaller[
```python
from statsmodels.stats.power import tt_ind_solve_power
```]
--
count:false
.smaller[
```python
# choose one to set as None
x = tt_ind_solve_power(effect_size=.05, # diff between means divided by std dev
                       nobs1=None,
                       alpha=.05,       # type 1 error
                       power=.95,       # 1 - type 2 error
                       ratio=1          # n_a : n_b
                  )
print(f'num observations needed: {np.ceil(x)}')
```
```
num observations needed: 10397.0
```]

---
# How many observations?

What happens to our power if we can only get 1000 observations?
--
count:false
.smaller[
```python
x = tt_ind_solve_power(effect_size=.05, # diff between means divided by std dev
                       nobs1=1000,
                       alpha=.05,       # type 1 error
                       power=None,       # 1 - type 2 error
                       ratio=1          # n_a : n_b
                  )
print(f'power: {x:0.2f}')
```
```
power: 0.20
```]

---
# Things to avoid

<br>
--
count:false
- **p-hacking:** keep trying comparisons till you find something that works

<br>
--
count:false
- **multiple tests:** the more tests you run, the more likely a Type 1 Error
--
count:false
    - Bonferonni correction: $\frac{\alpha}{m}$


---
# Statistically Significant?

--
count:false
.smallest[
[**The ASA Statement on p-Values: Context, Process, and Purpose**](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108) Wasserstein &amp; Lazar, 09 Jun 2016]

--
count:false
.smaller[
- Don’t base your conclusions solely on whether an association or effect was found to be “statistically significant” (i.e., the p-value passed some arbitrary threshold such as p &lt; 0.05).]

--
count:false
.smaller[
- Don’t believe an association/effect **exists** just because it **was statistically significant**.]

--
count:false
.smaller[
- Don’t believe an association/effect **is absent** just because it **was not stat. significant**.]

--
count:false
.smaller[
- Don’t believe that your p-value:
    1. gives the **probability that chance alone** produced the observed association/effect or 
    2. the probability that your **test hypothesis is true**.]

--
count:false
.smaller[
- Don’t conclude anything about **scientific or practical importance** based on statistical significance (or lack thereof).]

---
# Statistically Significant?

.smaller[
[**Moving to a World Beyond “p &lt; 0.05”**](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913) Wasserstein, Schirm &amp; Lazar, 20 Mar 2019]

--
count:false
.smaller[
- Don’t Say “Statistically Significant”
]

--
count:false
.smaller[
- “**A**ccept uncertainty. Be **t**houghtful, **o**pen, and **m**odest.” Remember “**ATOM**.”]


<br/>
--
count:false
.smaller[
- **A:** Seek better measures, more sensitive designs, larger samples
]

--
count:false
.smaller[
- **T:** Begin with clearly expressed objectives
- **T:** Ask "What are the practical implications?"
]

--
count:false
.smaller[
- **O:**: Be open/transparent in analysis and communication
]

--
count:false
.smaller[
- **M:** Accept limititaions, assumptions, reproduction, recognizing differences in stakes
]

---
# More Than 2 Groups

--
count:false
- ANOVA
    - need more stats than we have time for

--
count:false
- Multi-Armed Bandit (MAB)
    - can also be used for early stopping of experiment

---
#Multi-Armed Bandit

.center[![:scale 60%](images/slot_machines.jpg)]

--
count:false
**Question:** Which arm should we choose to pull?

---
# Greedy MAB

--
count:false
**greedy:** do something simple that heads towards the goal


--
count:false
1. pull best arm


--
count:false
- But what if there's a better choice, we just haven't seen it yet?

---
# Exploration vs Exploitation

<br>
--
count:false
- **Exploration:** There might be a better arm
    - keep choosing different arms randomly

<br>
--
count:false
- **Exploitation:** We want to make use of the best
    - keep pulling the best arm


---
# $\epsilon$-Greedy MAB

--
count:false
**greedy:** do something simple that heads towards the goal
--
count:false
- choose a small epsilon ($\epsilon$) between 0 and 1

--
count:false
1. generate random number between 0 and 1
--
count:false
1. if $\lt \epsilon$, choose arm randomly
--
count:false
1. if $\ge \epsilon$, choose best arm
--
count:false
1. GOTO 1

---
#MAB Example

--
count:false
- We have two ads
--
count:false
- We don't know how often each will lead to a response
--
count:false
- We need to decide which ad to add to each page request

--
count:false
```python
# creating two ads (distributions) with unknown response rate
np.random.seed(13)
ad_A = sp.stats.bernoulli(p=np.random.rand())
ad_B = sp.stats.bernoulli(p=np.random.rand())
```

--
count:false
- We'll use an $\epsilon$-greedy MAB to decide which ad to show

--
count:false
```python
# epsilon probability
epsilon = 0.40
```

---
#MAB Example

--
count:false
- Round 1 and Round 2
- Get an initial value for all arms

--
count:false
```python
pulls_A = [ad_A.rvs()]
```

--
count:false
```python
pulls_B = [ad_B.rvs()]
```

--
count:false
```python
pulls_A,pulls_B
```
```
([0], [1])
```
---
#MAB Example

--
count:false
- Round 3
- With probability $1-\epsilon$, choose the best arm (A)

--
count:false
```python
be_greedy = np.random.rand() > epsilon
be_greedy
```
```
True
```

--
count:false
```python
pulls_B.append(ad_B.rvs())
pulls_A,pulls_B
```
```
([0], [1, 0])
```
 
---
# MAB Example

.smallest[
```python
def mab(ad_A,ad_B,pulls_A,pulls_B,epsilon):
    be_greedy = np.random.rand() &gt; epsilon
    if not be_greedy: # randomly choose
        if np.random.rand() &lt; 0.5:
            pulls_A.append(ad_A.rvs())
            choice = 'A'
        else:
            pulls_B.append(ad_B.rvs())
            choice = 'B'
    else: # be greedy
        resp_A = sum(pulls_A) / len(pulls_A)
        resp_B = sum(pulls_B) / len(pulls_B)
        if resp_A &gt; resp_B:
            pulls_A.append(ad_A.rvs())
            choice = 'A'
        else:
            pulls_B.append(ad_B.rvs())
            choice = 'B'
    return pulls_A, pulls_B, be_greedy, choice
```]


---
#MAB Example

- Round 4

```python
pulls_A, pulls_B, be_greedy, choice = mab(ad_A,
                                          ad_B,
                                          pulls_A,
                                          pulls_B,
                                          epsilon)

print(be_greedy,choice,pulls_A, pulls_B)
```
--
count:false
```
True  B => 0.00:0.67  [0]                 , [1, 0, 1]
```

---
#MAB Example
.smallest[
```python
for i in range(10):
    pulls_A, pulls_B, be_greedy, choice = mab(ad_A,ad_B,pulls_A,pulls_B,epsilon)
    print(f'{str(be_greedy):5s} {choice} => '+
          f'{np.mean(pulls_A):0.2f}:{np.mean(pulls_B):0.2f} {str(pulls_A):20s}, {str(pulls_B):20s}')
```]
--
count:false
.smaller[
```
True  B => 0.00:0.50  [0]                 , [1, 0, 1, 0]
```]
--
count:false
.smaller[
```
False A => 0.50:0.50  [0, 1]              , [1, 0, 1, 0]
```]
--
count:false
.smaller[
```
True  B => 0.50:0.40  [0, 1]              , [1, 0, 1, 0, 0]
```]
--
count:false
.smaller[
```
True  A => 0.67:0.40  [0, 1, 1]           , [1, 0, 1, 0, 0]
```]
--
count:false
.smaller[
```
False A => 0.75:0.40  [0, 1, 1, 1]        , [1, 0, 1, 0, 0]
```]
--
count:false
.smaller[
```
True  A => 0.80:0.40  [0, 1, 1, 1, 1]     , [1, 0, 1, 0, 0]
```]
--
count:false
.smaller[
```
False B => 0.80:0.33  [0, 1, 1, 1, 1]     , [1, 0, 1, 0, 0, 0]
```]
--
count:false
.smaller[
```
False B => 0.80:0.43  [0, 1, 1, 1, 1]     , [1, 0, 1, 0, 0, 0, 1]
```]
--
count:false
.smaller[
```
False B => 0.80:0.50  [0, 1, 1, 1, 1]     , [1, 0, 1, 0, 0, 0, 1, 1]
```]
--
count:false
.smaller[
```
False B => 0.80:0.44  [0, 1, 1, 1, 1]     , [1, 0, 1, 0, 0, 0, 1, 1, 0]
```]

---
#MAB Example

- Which arm seems best?

.smaller[
```python
print(f'conversion rates: A: {np.mean(pulls_A):0.2f} B: {np.mean(pulls_B):0.2f}')
```]
```
conversion rates: A: 0.80 B: 0.44
```

<br/>
--
count:false
- Did we pick the best one?

.smaller[
```python
print(f'ground truth: A: {ad_A.pmf(1):0.2f} B: {ad_B.pmf(1):0.2f}')
```]
```
ground truth: A: 0.78 B: 0.24
```

---
#MAB Variations

--
count:false
- Thompson's Sampling: uses Baysian approach


--
count:false
- UCB1: maximize expected reward (for arm $j$: $\bar{x}_j + \sqrt{2\log t / n_j}$)

--
count:false
- ...

---
class:middle

# Questions?

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);

    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
